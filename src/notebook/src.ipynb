{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29873c6f",
   "metadata": {},
   "source": [
    "Hồi quy (Regression)\n",
    "\n",
    "Sử dụng tập dữ liệu california housing (file đính kèm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataSplitExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9aa00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'D:\\Nam4\\HK1\\Cloud_ML\\ThucHanh\\Buoi3\\src\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc85d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'randomSplit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Chia tập dư liệu thành train, validation, test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomSplit\u001b[49m([\u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m validation_df, test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'randomSplit'"
     ]
    }
   ],
   "source": [
    "#Chia tập dư liệu thành train, validation, test\n",
    "train_df, test_df = df.randomSplit([0.8,0.2], seed=42)\n",
    "validation_df, test_df = test_df.randomSplit([0.5,0.5], seed=42)\n",
    "print(f'Training set: {train_df.count()} samples')\n",
    "print(f'validation set: {validation_df.count()} samples')\n",
    "print(f'Test set: {test_df.count()} samples')\n",
    "\n",
    "#Hiển thị thống kê cơ bản của từng tập\n",
    "print('\\n Thống kê tập train:')\n",
    "display(train_df.describe())\n",
    "\n",
    "print('\\n Thống kê tập test:')\n",
    "display(test_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loại bỏ outlier\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "\n",
    "def remove_outliers_zscore(df, columns, threshold=3):\n",
    "    df_clean = df\n",
    "    for column in columns:\n",
    "        if column != 'MedHouseVal':\n",
    "            #Tính thống kê từ tập hiện tại\n",
    "            stats = df.select(\n",
    "                mean(col(column)).alias('mean'),\n",
    "                stddev(col(column)).alias('std')\n",
    "\n",
    "            ).collect()[0]\n",
    "\n",
    "            mean_val = stats['mean']\n",
    "            std_val = stats['std']\n",
    "\n",
    "            lower_bound = mean_val - threshold * std_val\n",
    "            upper_bound = mean_val + threshold * std_val\n",
    "\n",
    "            df_clean = df_clean.filter((col(column) >= lower_bound) & (col(column) <= upper_bound ))\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "train_df_clean = remove_outliers_zscore(train_df, train_df.columns)\n",
    "print(f'Train samples sau khi xử lý outliers: {train_df_clean.count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuẩn hóa và chuyển thành vector \n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "feature_columns = [col for col in train_df.columns if col != ' MedHouseVal']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = feature_columns,\n",
    "    outputCols='raw_feature'\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol = 'raw_feature',\n",
    "    outputCol ='features',\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "preprocessing_pipeline = Pipeline(stage=[assembler, scaler])\n",
    "preprocessing_model = preprocessing_pipeline.fit(train_df_clean)\n",
    "\n",
    "train_df_processed = preprocessing_model.transform(train_df_clean)\n",
    "validation_df_processed = preprocessing_model.transform(validation_df)\n",
    "\n",
    "display(train_df_processed.select('features', 'MedHouseVal').limit(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afe792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So sánh sự phân phối dữ liệu\n",
    "def compare_distributions(original_df, cleaned_df, column_name):\n",
    "    original_stats = original_df.select(mean(col(column_name)), stddev(col(column_name))).collect()[0]\n",
    "    cleaned_stats = cleaned_df.select(mean(col(column_name)), stddev(col(column_name))).collect()[0]\n",
    "\n",
    "    print(f'{column_name}:')\n",
    "    print(f'Original - Mean: {original_stats[0]:.4f}, Std: {original_stats[1]:.4f}')\n",
    "    print(f'Cleaned - Mean: {cleaned_stats[0]:.4f}, Std: {cleaned_stats[1]:.4f}')\n",
    "\n",
    "for feature in ['MedInc', 'AveRooms','HouseAge']:\n",
    "    compare_distributions(train_df, train_df_clean, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f678a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Huấn luyện và đánh giá\n",
    "from pyspark.ml.regression import LinearRegression, RandomForesRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol ='MedHouseVal',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse'\n",
    ")\n",
    "\n",
    "def train_and_evaluate_model(model, train_data, val_data, model_name):\n",
    "    trained_model = model.fit(train_data)\n",
    "    predictions = trained_model.transform(val_data)\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    evaluator.setMetricName('mae')\n",
    "    mae = evaluator.evaluate(predictions)\n",
    "    evaluator.setMetricName('r2')\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(f'{model_name} - Validation Metrics:')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'R^2:{r2:.4f}')\n",
    "    print('-' * 40)\n",
    "\n",
    "    return trained_model, predictions, {'RMSE': rmse, 'MAE': mae, 'R2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd49336",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featureCol='features', labelCol='MedHouseVal')\n",
    "lr_model, lr_predictions, lr_metrics = train_and_evaluate_model(\n",
    "    lr, train_df_processed, validation_df_processed, 'Linear Regression'\n",
    ")\n",
    "\n",
    "rf = RandomForesRegressor(featureCol='feature', labelCol='MedHouseVal', seed = 42)\n",
    "rf_model, rf_predictions, rf_metrics = train_and_evaluate_model(\n",
    "    rf, train_df_processed, validation_df_processed, 'Random Forest'\n",
    ")\n",
    "\n",
    "gbt = GBTRegressor(featureCol='feature', labelCol='MedHouseVal', seed = 42)\n",
    "gbt_model, gbt_predictions, gbt_metrics = train_and_evaluate_model(\n",
    "    gbt, train_df_processed, validation_df_processed, 'Gradient Boosting'\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5987a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGid(rf.numTrees, [50,100])\n",
    "              .addGrid(rf.maxDepth, [5,10])\n",
    "              .build())\n",
    "cross_val = CrossValidator(\n",
    "    estimator = rf,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cross_val.fit(train_df_processed)\n",
    "best_rf_model = cv_model.bestModel\n",
    "print(f'Best parameters: numTrees= {best_rf_model.getNumtrees}, maxDepth={best_rf_model.getMaxDepth()}')\n",
    "\n",
    "cv_predictions = best_rf_model.transform(validation_df_processed)\n",
    "cv_rmse = evaluator.evaluate(cv_predictions)\n",
    "print(f'Best Model RMSE on valiadation: {cv_rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2492e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đánh giá trên test set\n",
    "def final_evaluation(model, test_data, model_name):\n",
    "    \"\"\"Đánh giá cuối cùng trên tập test -  chỉ được chạy 1 lần\"\"\"\n",
    "    test_predictions = model.transform(test_data)\n",
    "\n",
    "    rmse = evaluator.setMetricName('rmse').evaluate(test_predictions)\n",
    "    mae = evaluator.setMetricName('mae').evaluate(test_predictions)\n",
    "    r2 = evaluator.setMetricName('r2').evaluate(test_predictions)\n",
    "\n",
    "    print(f'{model_name} - 'FINAL TEST METRICS:)\n",
    "    print(f'MESE: {rmse:.4f}')\n",
    "    print(f'MAE:{mae:.4f}')\n",
    "    print(f'R2: {r2:.4f}')\n",
    "\n",
    "    return test_predictions, {'RMSE':rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "#Đánh giá các models trên tập test\n",
    "print('Đnahs giá cuối cùng trên tập test:')\n",
    "print('=' * 50)\n",
    "\n",
    "lr_test_predictions, lr_test_metrics = final_evaluation(lr_model, test_df_processed, ' Linear Regression')\n",
    "rf_test_predictions, rf_test_metrics = final_evaluation(rf_model, test_df_processed, 'Random Forest')\n",
    "gbt_test_predictions, gbt_test_metrics = final_evaluation(gbt_model, test_df_processed, 'Gradient Boosting')\n",
    "cv_test_predictions, cv_test_metrics = final_evaluation(best_rf_model, test_df_processed, 'Random Forest CV')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc94776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đánh giá hiệu suất và trực quan dữ liệu\n",
    "from matplotlib import pyplot as plt \n",
    "import pandas as pd\n",
    "#So sánh tắt cả models\n",
    "models_comparison = {\n",
    "    'Linear Regression': lr_test_metrics,\n",
    "    'Random Forest': rf_test_metrics,\n",
    "    'Gradient Boosting': gbt_test_metrics,\n",
    "    'Random Forest CV': cv_test_metrics\n",
    "}\n",
    "\n",
    "#Hiển thị bằng so sánh\n",
    "comparison_df = pd.DataFrame(models_comparison).T\n",
    "print('\\n Bảng so sánh hiệu suất:')\n",
    "print(comparison_df)\n",
    "\n",
    "#viasualation\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "models = list(models_comparison.keys())\n",
    "rmse_scores = [metrics['RMSE'] for metrics in models_comparison.values()]\n",
    "plt.bar(models, rmse_scores, color=['skyblue','lightgreen','lightcoral','gold'])\n",
    "plt.title('RMSE Comparison on test set')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1,2,3)\n",
    "r2_scores =[metrics['R2'] for metrics in models_comparison.value()]\n",
    "plt.bar(models, r2_scores, color=['skyblue','lightgreen','lightcoral','gold'])\n",
    "plt.title('R2 Comparison on Test set')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
